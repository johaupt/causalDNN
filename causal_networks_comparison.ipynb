{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a standard multi-layer perceptron. Classification only requires a change to the output activation from None to sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super(nnet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.layer_sizes = hidden_layer_sizes\n",
    "        self.iter = 0\n",
    "        \n",
    "        hidden_layer_sizes = hidden_layer_sizes + [1] # Output layer\n",
    "        first_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n",
    "        self.layers = nn.ModuleList(\n",
    "            [first_layer] +\\\n",
    "            [nn.Linear(input_, output_)\n",
    "             for input_, output_ in \n",
    "             zip(hidden_layer_sizes, hidden_layer_sizes[1:])])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, data_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self._train_iteration(data_loader)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"loss: {loss}\")\n",
    "                \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X = Variable(X, requires_grad=True)\n",
    "            y = Variable(y, requires_grad=True)\n",
    "                      \n",
    "            pred = self(X)\n",
    "            loss = ((y - pred)**2).mean()\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "               \n",
    "        return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generating process works as follows (story just for reference):\n",
    "Revenue ($y_i$) depends on some characteristics $X_i$ of the customer i. Customers are given a coupon (treatment $g_i$) with 50% probability . Customer who receive a coupon will spend more or less money (treatment effect $\\tau_i$) depending linearly on their characteristics.\n",
    "\n",
    "$$y_i = X_i ^\\top \\beta_X + g_i \\cdot (\\tau_0 + X_i ^\\top \\beta_{\\tau} + \\epsilon^{\\tau}_i) + \\epsilon_i$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\\epsilon_i \\sim \\text{Normal}(mean = 0, std = 0.1)$$\n",
    "\n",
    "$$g_i \\sim \\text{Bernoulli}(p=0.5)$$\n",
    "$$\\epsilon^{\\tau}_i \\sim \\text{Normal}(mean = 0, std = 0.001)$$\n",
    "\n",
    "I think there is merit to the assumption that the true reponse model is often more complex than the model behind the heterogeneity of treatment effects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment(n_obs,n_var, tau=None):\n",
    "    X = np.random.multivariate_normal(\n",
    "             np.zeros(n_var),\n",
    "             np.eye(n_var),\n",
    "             n_obs\n",
    "             )\n",
    "    \n",
    "    \n",
    "    # Linear effects\n",
    "    beta     = np.random.normal(scale=0.1, size=n_var)\n",
    "    # Non-linear effects (optional)\n",
    "    beta_X2  = np.random.normal(loc=0, scale=0.1, size=n_var)\n",
    "    # Linear effects on treatment effect\n",
    "    beta_tau = np.random.normal(loc=0,scale=0.01, size=n_var)\n",
    "    # Baseline treatment effect\n",
    "    tau_zero = np.random.normal(0.1,0.01)\n",
    "    \n",
    "    g = np.hstack([np.ones(n_obs//2), np.zeros(n_obs//2)])\n",
    "        #np.random.binomial(1,0.5,size=n_obs)\n",
    "        \n",
    "    if tau is None:\n",
    "        tau = tau_zero + np.dot(X,beta_tau) + np.random.normal(scale=0.001, size=n_obs)\n",
    "        \n",
    "    y = np.dot(X,beta) +\\\n",
    "        np.dot(np.power(X,2),beta_X2) +\\\n",
    "        g * tau + np.random.normal(scale=0.1, size=n_obs)\n",
    "    \n",
    "    return X, y, g, tau, beta, beta_X2, tau_zero, beta_tau\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataloader is currently a random sampler (see below) that returns $X_i$, $y_i$ and $g_i$ for a batch. \n",
    "\n",
    "TODO: I think a batch sampler stratified on $g$ makes more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentData(Dataset):\n",
    "    def __init__(self, X, y, g):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.g = g\n",
    "        \n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return X[idx,:], y[idx], g[idx]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, g, tau, coef, coef_x2, tau_zero, coef_tau = generate_experiment(15000,10, tau=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation in the simulation context, I assume that the treatment effects are known and calculate the accuracy on the model in estimating the treatment effects on a holdout validation set. In practice, the true treatment effects are unknown, of course, so holdout evaluation is an open question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.30378227001716507, 0.38715602598402266)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATE summary statistics. These should be stable to confirm that the info-noise ratio in the data is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline treatment effect (True ATE):0.09941928491355384\n",
      "Sample treatment effect (ITE Mean, ITE Std.): (0.09968136775466813, 0.02549210466909146)\n",
      "Empirical ATE: 0.09203177699345721\n"
     ]
    }
   ],
   "source": [
    "# True ATE and standard deviation of individual treatment effects\n",
    "print(f\"Baseline treatment effect (True ATE):{tau_zero}\")\n",
    "print(f\"Sample treatment effect (ITE Mean, ITE Std.): {np.mean(tau), np.std(tau)}\")\n",
    "print(f\"Empirical ATE: {np.mean(y[g==1]) - np.mean(y[g==0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_val, y, y_val, g, g_val, tau, tau_val = train_test_split(X,y,g,tau, stratify=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE for treatment effect prediction on validation data:      0.0007049108390374096\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline MSE for treatment effect prediction on validation data:\\\n",
    "      {np.mean((tau_val - (np.mean(y[g==1]) - np.mean(y[g==0])))**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ExperimentData(X,y,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch ATE causal net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The two-model approach estimates two distinct models on the response variable, one for each group. The difference between the two estimates for the same observations is the treatment effect for a single observation. Disjoint estimation may result in models that are not well calibrated.\n",
    "\n",
    "Solution: Train a neural network to estimate the treatment effect directly. This is not possible for a single observation (-> fundamental problem of causal inference):\n",
    "\n",
    "$$ r_i = \\hat{\\tau}_i - \\tau_i, $$\n",
    "where $\\tau_i$ is of course unknown. \n",
    "\n",
    "We can however evaluate the total error for groups of observations $i \\in 1,\\ldots,N$:\n",
    "\n",
    "$$ \\sum^N r_i = \\sum^N \\hat{\\tau}_i - \\tau_i = \\sum^N \\hat{\\tau}_i - \\sum^N \\tau_i$$\n",
    "\n",
    "\n",
    "The sum treatment effect is only weakly informative for the treatment effect of a single observation and estimating the overall sum of treatment effects leaves too many degrees of freedom for the treatment effect of each observations. By using mini-batches instead, the summed individual treatment effects need to be correct not only for the population N, but also for each subset of the population $M \\in N$.\n",
    "\n",
    "The trick is in shuffling and reshuffling the observation between the batches. On the full sample, the model can get away with predicting the ATE for each observation. For the ATE *in each subset* to be correct, the model needs to predict the individual treatment effects correctly. \n",
    "\n",
    "TODO: Show this formally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalnet1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.net = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        treatment_effect = self.net(X)\n",
    "        return treatment_effect\n",
    "    \n",
    "    def train(self, data_loader, epochs, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            ATE, ATE_hat, loss, g_ratio = self._train_iteration(data_loader)\n",
    "            self.loss.append(loss)\n",
    "            if epoch % 5 == 0:\n",
    "                if validation_data is not None:\n",
    "                    tau_hat = self.net(validation_data['X'])\n",
    "                    val_loss = (validation_data['tau'] - tau_hat).pow(2).mean().detach().numpy()\n",
    "                    self.val_loss.append(val_loss)\n",
    "                    print(f\"val_loss: {val_loss}, ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "                else:\n",
    "                    print(f\"ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "            \n",
    "    \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X0 = Variable(X[g==0,:])\n",
    "            y0 = Variable(y[g==0])\n",
    "            \n",
    "            X1 = Variable(X[g==1,:])\n",
    "            y1 = Variable(y[g==1])\n",
    "            \n",
    "            g_ratio = g.float().mean()\n",
    "            \n",
    "#             response0 = self.net0(X0)\n",
    "#             loss0 = ((y0 - response0)**2).mean()\n",
    "            \n",
    "#             response1 = self.net1(X1)\n",
    "#             loss1 = ((y1 - response1)**2).mean()\n",
    "\n",
    "            treatment_effect = self.net(X)\n",
    "            \n",
    "            ATE_hat = treatment_effect.mean()\n",
    "            ATE     = y1.mean() - y0.mean()\n",
    "            loss_treatment = (ATE - ATE_hat)**2            \n",
    "            \n",
    "            \n",
    "#             if i % 2==0:\n",
    "#                 self.net0.zero_grad()\n",
    "#                 loss =  0*loss0 + loss_treatment\n",
    "#                 loss.backward()\n",
    "#                 optim0.step()\n",
    "#             else:\n",
    "            self.net.zero_grad()\n",
    "            loss =  loss_treatment\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "                \n",
    "            \n",
    "        return ATE.detach().numpy(), ATE_hat.detach().numpy(), loss.detach().numpy(), g_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a FFNN without hidden layers as equivalent to a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = causalnet1(10, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My intuition about the batch size is tradeoff: Too large and the signal from each observation becomes too small and predicting the ATE is the dominant strategy. Too small and the training becomes unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: A dynamic decrease in batch size could provide more and more information given that the model is stable enough to create decent estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Ideally, the data loader will pass bs/2 observations of each the treatment and the control group, but this will take some coding, so I ignore it for now. For small batch sizes, it's currently possible that not both groups are present in the batch so the loss will return NaN and training fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low learning rate is possibly necessary to stabilize training given the noise in the ATE within each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(cnn1.net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim0 = Adam(cnn.net0.parameters(), lr=0.001)\n",
    "# optim1 = Adam(cnn.net1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.006031661760061979, ATE: 0.08883598446846008, ATE_hat: 0.09818439930677414, loss: 8.739285840420052e-05, balance: 0.5120000243186951\n",
      "val_loss: 0.007055092602968216, ATE: 0.09039746224880219, ATE_hat: 0.10067525506019592, loss: 0.00010563302203081548, balance: 0.5040000081062317\n",
      "val_loss: 0.005625902209430933, ATE: 0.12690168619155884, ATE_hat: 0.09595236927270889, loss: 0.0009578602039255202, balance: 0.492000013589859\n",
      "val_loss: 0.007948490791022778, ATE: 0.1316843032836914, ATE_hat: 0.09030191600322723, loss: 0.0017125019803643227, balance: 0.4440000057220459\n"
     ]
    }
   ],
   "source": [
    "cnn1.train(data_loader, epochs=20, validation_data = {\"X\":torch.tensor(X_val).float(), \n",
    "                                              \"tau\":torch.Tensor(tau_val).float()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn1.val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks without hidden layer, we can compare the coefficients to the known true coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ATE is usually approximately correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09941928491355384"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09372278], dtype=float32)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn1.net.parameters())[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated coefficients often don't capture the direction and even approximate size of the effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01181628, -0.00056324, -0.00038409, -0.0026142 ,  0.00355688,\n",
       "       -0.01024895,  0.00623549,  0.00018308, -0.00534166, -0.01784753])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03630454,  0.01598906, -0.0417923 ,  0.01154869, -0.0105392 ,\n",
       "        -0.02764627,  0.01178028,  0.01612512, -0.02893417, -0.0010893 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn1.net.parameters())[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted treatment effects are usually not a accurate estimate of the true individual treatment effects, but correlation is at least positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn1(torch.tensor(X).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09954795307329109 0.02537136176155067\n",
      "0.09374455 0.074115425\n",
      "[[1.         0.46754787]\n",
      " [0.46754787 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tau.mean(), tau.std())\n",
    "print(pred.mean(), pred.std())\n",
    "print(np.corrcoef(tau,pred.flatten(), rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23299932, 0.04181808, 0.06345747, 0.00100724, 0.12652723,\n",
       "       0.05745436, 0.07396063, 0.2554624 , 0.11520694, 0.00644137],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12381572, 0.10763013, 0.04988213, 0.0977745 , 0.12228992,\n",
       "       0.13137319, 0.1038555 , 0.10988424, 0.09399612, 0.09210034])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addditive two-model approach / residual network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Again estimate two networks jointly. Decompose the response estimate for treated observations into the response without treatment and the treatment effect. \n",
    "\n",
    "$$ \\hat{y}_t = f_R(X_t) + f_T(X_t) \\\\\n",
    "   \\hat{y}_t = f_R(X_c)$$\n",
    "\n",
    "One network $f_R$ predicts the response without treatment for each customer (treatment and control), the second network $f_T$ estimates the treatment effect. The loss for the first network is the response prediction MSE over all observations $$\\frac{1}{N}\\sum([\\hat{y_t};\\hat{y_c}] - [y_t;y_c])^2$$ the loss for the second network is the response prediction MSE for *only the treated group*, i.e. $$\\frac{1}{N_T}\\sum(\\hat{y_t} - y_t)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: This seems related to an approach called *covariate transformation* in the uplift literature. For covariate transformation, we estimate the outcome $y_i$ on a set of variables $[X_i, t_i \\cdot X_i]$, where $t_i \\cdot X_i$ is the interaction between the treatment indicator $t_i \\in {0;1}$ and observed variables $X$. \n",
    "\n",
    "TODO: For the linear regression model, this is equivalent to including each variable-treatment interaction term (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalnet2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.net0 = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.net1 = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net1(X)\n",
    "    \n",
    "    def train(self, data_loader, epochs, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            ATE, ATE_hat, loss, g_ratio = self._train_iteration(data_loader)\n",
    "            self.loss.append(loss)\n",
    "            if epoch % 5 == 0:\n",
    "                if validation_data is not None:\n",
    "                    tau_hat = self.net1(validation_data['X'])\n",
    "                    val_loss = (validation_data['tau'] - tau_hat).pow(2).mean().detach().numpy()\n",
    "                    self.val_loss.append(val_loss)\n",
    "                    print(f\"val_loss: {val_loss}, ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "                else:\n",
    "                    print(f\"ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "            \n",
    "    \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X0 = Variable(X[g==0,:])\n",
    "            y0 = Variable(y[g==0])\n",
    "            \n",
    "            X1 = Variable(X[g==1,:])\n",
    "            y1 = Variable(y[g==1])\n",
    "            \n",
    "            g_ratio = g.float().mean()\n",
    "            \n",
    "            response0 = self.net0(X0)\n",
    "            loss0 = ((y0 - response0).pow(2)).mean()\n",
    "            \n",
    "            response1 = self.net0(X1) + self.net1(X1)\n",
    "            loss1 = ((y1 - response1).pow(2)).mean()\n",
    "            \n",
    "            loss = loss0 + loss1\n",
    "            \n",
    "            ATE_hat = response1.mean() - response0.mean()\n",
    "            ATE     = y1.mean() - y0.mean()\n",
    "            loss_treatment = (ATE - ATE_hat).pow(2)            \n",
    "            \n",
    "            self.net0.zero_grad()\n",
    "            loss0.backward()\n",
    "            optim0.step()\n",
    "\n",
    "            self.net1.zero_grad()\n",
    "            loss1.backward()\n",
    "            optim1.step()\n",
    "                \n",
    "            \n",
    "        return ATE.detach().numpy(), ATE_hat.detach().numpy(), loss.detach().numpy(), g_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a FFNN without hidden layers as equivalent to a linear regression model. Note that the true model for y is non-linear, while the true model for the treatment effect is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = causalnet2(10, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim0 = Adam(cnn2.net0.parameters(), lr=0.001)\n",
    "optim1 = Adam(cnn2.net1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.2135770320892334, ATE: 0.18655477464199066, ATE_hat: 0.19107061624526978, loss: 0.5158156752586365, balance: 0.5\n",
      "val_loss: 0.017792565748095512, ATE: -0.034338489174842834, ATE_hat: 0.027360394597053528, loss: 0.202156662940979, balance: 0.48245614767074585\n",
      "val_loss: 0.0017366966931149364, ATE: 0.0642431378364563, ATE_hat: 0.05068811774253845, loss: 0.162737175822258, balance: 0.41228070855140686\n",
      "val_loss: 0.001481969142332673, ATE: 0.026723504066467285, ATE_hat: 0.026685744524002075, loss: 0.11837701499462128, balance: 0.45614033937454224\n",
      "val_loss: 0.0017251571407541633, ATE: 0.07240289449691772, ATE_hat: 0.08606421947479248, loss: 0.17076972126960754, balance: 0.5\n",
      "val_loss: 0.0016285869060084224, ATE: 0.17566508054733276, ATE_hat: 0.06951329112052917, loss: 0.15856397151947021, balance: 0.5526315569877625\n",
      "val_loss: 0.0017163169104605913, ATE: 0.13948842883110046, ATE_hat: 0.13218584656715393, loss: 0.158387690782547, balance: 0.5175438523292542\n",
      "val_loss: 0.0015009131748229265, ATE: 0.008832663297653198, ATE_hat: 0.04156199097633362, loss: 0.22436577081680298, balance: 0.5087719559669495\n",
      "val_loss: 0.0018471891526132822, ATE: 0.20117761194705963, ATE_hat: 0.10038845241069794, loss: 0.14521852135658264, balance: 0.5350877046585083\n",
      "val_loss: 0.0016961604123935103, ATE: -0.018645867705345154, ATE_hat: 0.049415647983551025, loss: 0.11788711696863174, balance: 0.5\n",
      "val_loss: 0.0015047002816572785, ATE: 0.11024054884910583, ATE_hat: 0.12398874759674072, loss: 0.0774020105600357, balance: 0.5614035129547119\n",
      "val_loss: 0.001628166064620018, ATE: 0.026520878076553345, ATE_hat: 0.05099445581436157, loss: 0.15106201171875, balance: 0.5614035129547119\n",
      "val_loss: 0.001645846525207162, ATE: 0.023354053497314453, ATE_hat: 0.07698756456375122, loss: 0.1276114583015442, balance: 0.6052631735801697\n",
      "val_loss: 0.0015664191450923681, ATE: 0.06620272994041443, ATE_hat: 0.06919234991073608, loss: 0.09604902565479279, balance: 0.5087719559669495\n",
      "val_loss: 0.001730429008603096, ATE: 0.0345359742641449, ATE_hat: 0.0501275360584259, loss: 0.12943047285079956, balance: 0.5350877046585083\n",
      "val_loss: 0.0015246811090037227, ATE: 0.046597838401794434, ATE_hat: 0.06610018014907837, loss: 0.13773463666439056, balance: 0.5438596606254578\n",
      "val_loss: 0.0019435911672189832, ATE: 0.15459109842777252, ATE_hat: 0.16622301936149597, loss: 0.14003831148147583, balance: 0.45614033937454224\n",
      "val_loss: 0.0015330180758610368, ATE: 0.14012262225151062, ATE_hat: 0.17622607946395874, loss: 0.1120634526014328, balance: 0.48245614767074585\n",
      "val_loss: 0.001549965818412602, ATE: 0.10570001602172852, ATE_hat: 0.1222076267004013, loss: 0.13965517282485962, balance: 0.4385964870452881\n",
      "val_loss: 0.0017926155123859644, ATE: -0.029206037521362305, ATE_hat: 0.0686250776052475, loss: 0.15106521546840668, balance: 0.5175438523292542\n",
      "val_loss: 0.001736525329761207, ATE: 0.09535858035087585, ATE_hat: 0.06106129288673401, loss: 0.12616018950939178, balance: 0.5350877046585083\n",
      "val_loss: 0.0015807872405275702, ATE: 0.03959193825721741, ATE_hat: 0.0659455955028534, loss: 0.10782542824745178, balance: 0.4912280738353729\n",
      "val_loss: 0.0016753282397985458, ATE: 0.018269836902618408, ATE_hat: 0.05236572027206421, loss: 0.12741293013095856, balance: 0.5263158082962036\n",
      "val_loss: 0.001479148748330772, ATE: 0.14986762404441833, ATE_hat: 0.1129213273525238, loss: 0.15413305163383484, balance: 0.4736842215061188\n",
      "val_loss: 0.001476727775298059, ATE: 0.06314916908740997, ATE_hat: 0.04800605773925781, loss: 0.11581797152757645, balance: 0.5438596606254578\n",
      "val_loss: 0.001728893374092877, ATE: 0.1214471310377121, ATE_hat: 0.09689928591251373, loss: 0.16050611436367035, balance: 0.6140350699424744\n",
      "val_loss: 0.001572599052451551, ATE: 0.0683726966381073, ATE_hat: 0.02068600058555603, loss: 0.10235556960105896, balance: 0.44736841320991516\n",
      "val_loss: 0.0014858194626867771, ATE: 0.05944600701332092, ATE_hat: 0.06186860799789429, loss: 0.1164393201470375, balance: 0.4385964870452881\n",
      "val_loss: 0.0015265278052538633, ATE: 0.03888443112373352, ATE_hat: 0.053599536418914795, loss: 0.11666607856750488, balance: 0.41228070855140686\n",
      "val_loss: 0.0016991826705634594, ATE: -0.07373359799385071, ATE_hat: 0.09270280599594116, loss: 0.19852395355701447, balance: 0.4912280738353729\n",
      "val_loss: 0.001728007453493774, ATE: 0.07132300734519958, ATE_hat: 0.0024548470973968506, loss: 0.10124798119068146, balance: 0.5438596606254578\n",
      "val_loss: 0.0017630044603720307, ATE: 0.13165472447872162, ATE_hat: 0.13665331900119781, loss: 0.0922539234161377, balance: 0.5263158082962036\n",
      "val_loss: 0.001521107624284923, ATE: 0.09926624596118927, ATE_hat: 0.0698544830083847, loss: 0.14976200461387634, balance: 0.5\n",
      "val_loss: 0.0015017807018011808, ATE: 0.11369214951992035, ATE_hat: 0.08766970038414001, loss: 0.09224720299243927, balance: 0.5438596606254578\n",
      "val_loss: 0.0018145046196877956, ATE: 0.11464333534240723, ATE_hat: 0.05846595764160156, loss: 0.1467813104391098, balance: 0.5526315569877625\n",
      "val_loss: 0.0016895511653274298, ATE: 0.09362438321113586, ATE_hat: 0.07468771934509277, loss: 0.16497191786766052, balance: 0.5701754093170166\n",
      "val_loss: 0.0016406190115958452, ATE: 0.12439389526844025, ATE_hat: 0.09603738784790039, loss: 0.14508527517318726, balance: 0.37719297409057617\n",
      "val_loss: 0.0018669080454856157, ATE: 0.09225931763648987, ATE_hat: 0.08211119472980499, loss: 0.14014321565628052, balance: 0.42105263471603394\n",
      "val_loss: 0.001690225093625486, ATE: 0.18480485677719116, ATE_hat: 0.18168367445468903, loss: 0.1095670759677887, balance: 0.4736842215061188\n",
      "val_loss: 0.001535750925540924, ATE: 0.20444509387016296, ATE_hat: 0.178988978266716, loss: 0.12141844630241394, balance: 0.5087719559669495\n"
     ]
    }
   ],
   "source": [
    "cnn2.train(data_loader, epochs=200, validation_data = {\"X\":torch.tensor(X_val).float(), \n",
    "                                              \"tau\":torch.Tensor(tau_val).float()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn2.val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks without hidden layer, we can compare the coefficients to the known true coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09941928491355384"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09622353], dtype=float32)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn2.net1.parameters())[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated coefficients usually capture the direction and even approximate size of the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01181628, -0.00056324, -0.00038409, -0.0026142 ,  0.00355688,\n",
       "       -0.01024895,  0.00623549,  0.00018308, -0.00534166, -0.01784753])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01858542,  0.00478654, -0.01331575,  0.006138  ,  0.00356346,\n",
       "        -0.01191409,  0.00573493, -0.00151849, -0.00746163, -0.02146538]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn2.net1.parameters())[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted treatment effects are usually not a accurate estimate of the true individual treatment effects, but correlation is often high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn2.net1(torch.tensor(X).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09954795307329109 0.02537136176155067\n",
      "0.09634175 0.035682973\n",
      "[[1.         0.86795479]\n",
      " [0.86795479 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tau.mean(), tau.std())\n",
    "print(pred.mean(), pred.std())\n",
    "print(np.corrcoef(tau,pred.flatten(), rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12510437, 0.12058824, 0.05378344, 0.08770087, 0.14319935,\n",
       "       0.12863287, 0.1067209 , 0.12998931, 0.10424721, 0.07129309],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12381572, 0.10763013, 0.04988213, 0.0977745 , 0.12228992,\n",
       "       0.13137319, 0.1038555 , 0.10988424, 0.09399612, 0.09210034])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
