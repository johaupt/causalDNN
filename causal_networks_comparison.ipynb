{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a standard multi-layer perceptron. Classification only requires a change to the output activation from None to sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super(nnet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.layer_sizes = hidden_layer_sizes\n",
    "        self.iter = 0\n",
    "        \n",
    "        hidden_layer_sizes = hidden_layer_sizes + [1] # Output layer\n",
    "        first_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n",
    "        self.layers = nn.ModuleList(\n",
    "            [first_layer] +\\\n",
    "            [nn.Linear(input_, output_)\n",
    "             for input_, output_ in \n",
    "             zip(hidden_layer_sizes, hidden_layer_sizes[1:])])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, data_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self._train_iteration(data_loader)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"loss: {loss}\")\n",
    "                \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X = Variable(X, requires_grad=True)\n",
    "            y = Variable(y, requires_grad=True)\n",
    "                      \n",
    "            pred = self(X)\n",
    "            loss = ((y - pred)**2).mean()\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "               \n",
    "        return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generating process works as follows (story just for reference):\n",
    "Revenue ($y_i$) depends on some characteristics $X_i$ of the customer i. Customers are given a coupon (treatment $g_i$) with 50% probability . Customer who receive a coupon will spend more or less money (treatment effect $\\tau_i$) depending linearly on their characteristics.\n",
    "\n",
    "$$y_i = X_i ^\\top \\beta_X + g_i \\cdot (\\tau_0 + X_i ^\\top \\beta_{\\tau} + \\epsilon^{\\tau}_i) + \\epsilon_i$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\\epsilon_i \\sim \\text{Normal}(mean = 0, std = 0.1)$$\n",
    "\n",
    "$$g_i \\sim \\text{Bernoulli}(p=0.5)$$\n",
    "$$\\epsilon^{\\tau}_i \\sim \\text{Normal}(mean = 0, std = 0.001)$$\n",
    "\n",
    "I think there is merit to the assumption that the true reponse model is often more complex than the model behind the heterogeneity of treatment effects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment(n_obs,n_var, tau=None):\n",
    "    X = np.random.multivariate_normal(\n",
    "             np.zeros(n_var),\n",
    "             np.eye(n_var),\n",
    "             n_obs\n",
    "             )\n",
    "    \n",
    "    \n",
    "    # Linear effects\n",
    "    beta     = np.random.normal(loc=0, scale=0.1, size=n_var)\n",
    "    # Non-linear effects (optional)\n",
    "    beta_X2  = np.random.normal(loc=0, scale=0.1, size=n_var)\n",
    "    # Linear effects on treatment effect\n",
    "    beta_tau = np.random.normal(loc=0, scale=0.01, size=n_var)\n",
    "    # Baseline treatment effect\n",
    "    tau_zero = np.random.normal(0.1,0.01)\n",
    "    \n",
    "    g = np.hstack([np.ones(n_obs//2), np.zeros(n_obs//2)])\n",
    "        #np.random.binomial(1,0.5,size=n_obs)\n",
    "        \n",
    "    if tau is None:\n",
    "        tau = tau_zero + np.dot(X,beta_tau) + np.random.normal(scale=0.001, size=n_obs)\n",
    "        \n",
    "    y = np.dot(X,beta) +\\\n",
    "        np.dot(np.power(X,2),beta_X2) +\\\n",
    "        g * tau + np.random.normal(scale=0.1, size=n_obs)\n",
    "    \n",
    "    return X, y, g, tau, beta, beta_X2, tau_zero, beta_tau\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataloader is currently a random sampler (see below) that returns $X_i$, $y_i$ and $g_i$ for a batch. \n",
    "\n",
    "TODO: I think a batch sampler stratified on $g$ makes more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentData(Dataset):\n",
    "    def __init__(self, X, y, g):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.g = g\n",
    "        \n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return X[idx,:], y[idx], g[idx]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, g, tau, coef, coef_x2, tau_zero, coef_tau = generate_experiment(15000,10, tau=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation in the simulation context, I assume that the treatment effects are known and calculate the accuracy on the model in estimating the treatment effects on a holdout validation set. In practice, the true treatment effects are unknown, of course, so holdout evaluation is an open question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3173589366841803, 0.48587312119133874)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATE summary statistics. These should be stable to confirm that the info-noise ratio in the data is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline treatment effect (True ATE):0.09801189472366993\n",
      "Sample treatment effect (ITE Mean, ITE Std.): (0.09784800730566624, 0.02774634138414201)\n",
      "Empirical ATE: 0.09049648289956486\n"
     ]
    }
   ],
   "source": [
    "# True ATE and standard deviation of individual treatment effects\n",
    "print(f\"Baseline treatment effect (True ATE):{tau_zero}\")\n",
    "print(f\"Sample treatment effect (ITE Mean, ITE Std.): {np.mean(tau), np.std(tau)}\")\n",
    "print(f\"Empirical ATE: {np.mean(y[g==1]) - np.mean(y[g==0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_val, y, y_val, g, g_val, tau, tau_val = train_test_split(X,y,g,tau, stratify=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE for treatment effect prediction on validation data:      0.0007715055326428349\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline MSE for treatment effect prediction on validation data:\\\n",
    "      {np.mean((tau_val - (np.mean(y[g==1]) - np.mean(y[g==0])))**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ExperimentData(X,y,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch ATE causal net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The two-model approach estimates two distinct models on the response variable, one for each group. The difference between the two estimates for the same observations is the treatment effect for a single observation. Disjoint estimation may result in models that are not well calibrated.\n",
    "\n",
    "Solution: Train a neural network to estimate the treatment effect directly. This is not possible for a single observation (-> fundamental problem of causal inference):\n",
    "\n",
    "$$ r_i = \\hat{\\tau}_i - \\tau_i, $$\n",
    "where $\\tau_i$ is of course unknown. \n",
    "\n",
    "We can however evaluate the total error for groups of observations $i \\in 1,\\ldots,N$:\n",
    "\n",
    "$$ \\sum^N r_i = \\sum^N \\hat{\\tau}_i - \\tau_i = \\sum^N \\hat{\\tau}_i - \\sum^N \\tau_i$$\n",
    "\n",
    "\n",
    "The sum treatment effect is only weakly informative for the treatment effect of a single observation and estimating the overall sum of treatment effects leaves too many degrees of freedom for the treatment effect of each observations. By using mini-batches instead, the summed individual treatment effects need to be correct not only for the population N, but also for each subset of the population $M \\in N$.\n",
    "\n",
    "The trick is in shuffling and reshuffling the observation between the batches. On the full sample, the model can get away with predicting the ATE for each observation. For the ATE *in each subset* to be correct, the model needs to predict the individual treatment effects correctly. \n",
    "\n",
    "TODO: Show this formally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalnet1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.net = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        treatment_effect = self.net(X)\n",
    "        return treatment_effect\n",
    "    \n",
    "    def train(self, data_loader, epochs, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            ATE, ATE_hat, loss, g_ratio = self._train_iteration(data_loader)\n",
    "            self.loss.append(loss)\n",
    "            if epoch % 5 == 0:\n",
    "                if validation_data is not None:\n",
    "                    tau_hat = self.net(validation_data['X'])\n",
    "                    val_loss = (validation_data['tau'] - tau_hat).pow(2).mean().detach().numpy()\n",
    "                    self.val_loss.append(val_loss)\n",
    "                    print(f\"val_loss: {val_loss}, ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "                else:\n",
    "                    print(f\"ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "            \n",
    "    \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X0 = Variable(X[g==0,:])\n",
    "            y0 = Variable(y[g==0])\n",
    "            \n",
    "            X1 = Variable(X[g==1,:])\n",
    "            y1 = Variable(y[g==1])\n",
    "            \n",
    "            g_ratio = g.float().mean()\n",
    "            \n",
    "#             response0 = self.net0(X0)\n",
    "#             loss0 = ((y0 - response0)**2).mean()\n",
    "            \n",
    "#             response1 = self.net1(X1)\n",
    "#             loss1 = ((y1 - response1)**2).mean()\n",
    "\n",
    "            treatment_effect = self.net(X)\n",
    "            \n",
    "            ATE_hat = treatment_effect.mean()\n",
    "            ATE     = y1.mean() - y0.mean()\n",
    "            loss_treatment = (ATE - ATE_hat)**2            \n",
    "            \n",
    "            \n",
    "#             if i % 2==0:\n",
    "#                 self.net0.zero_grad()\n",
    "#                 loss =  0*loss0 + loss_treatment\n",
    "#                 loss.backward()\n",
    "#                 optim0.step()\n",
    "#             else:\n",
    "            self.net.zero_grad()\n",
    "            loss =  loss_treatment\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "                \n",
    "            \n",
    "        return ATE.detach().numpy(), ATE_hat.detach().numpy(), loss.detach().numpy(), g_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a FFNN without hidden layers as equivalent to a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = causalnet1(10, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My intuition about the batch size is tradeoff: Too large and the signal from each observation becomes too small and predicting the ATE is the dominant strategy. Too small and the training becomes unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: A dynamic decrease in batch size could provide more and more information given that the model is stable enough to create decent estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Ideally, the data loader will pass bs/2 observations of each the treatment and the control group, but this will take some coding, so I ignore it for now. For small batch sizes, it's currently possible that not both groups are present in the batch so the loss will return NaN and training fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low learning rate is possibly necessary to stabilize training given the noise in the ATE within each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(cnn1.net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim0 = Adam(cnn.net0.parameters(), lr=0.001)\n",
    "# optim1 = Adam(cnn.net1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.3223493695259094, ATE: 0.1355738788843155, ATE_hat: -0.0003725823189597577, loss: 0.01848144270479679, balance: 0.5120000243186951\n",
      "val_loss: 0.28972387313842773, ATE: 0.10702365636825562, ATE_hat: 0.06103650480508804, loss: 0.002114818198606372, balance: 0.527999997138977\n",
      "val_loss: 0.2596900761127472, ATE: 0.09395122528076172, ATE_hat: 0.07540743798017502, loss: 0.0003438720596022904, balance: 0.5199999809265137\n",
      "val_loss: 0.21470093727111816, ATE: 0.15928679704666138, ATE_hat: 0.12285935133695602, loss: 0.0013269587652757764, balance: 0.5400000214576721\n",
      "val_loss: 0.1785833090543747, ATE: 0.04732125997543335, ATE_hat: 0.06548316776752472, loss: 0.00032985489815473557, balance: 0.5239999890327454\n",
      "val_loss: 0.15420444309711456, ATE: 0.1452709585428238, ATE_hat: 0.07246974855661392, loss: 0.005300016142427921, balance: 0.46799999475479126\n",
      "val_loss: 0.13500168919563293, ATE: 0.01909017562866211, ATE_hat: 0.03405449911952019, loss: 0.00022393098333850503, balance: 0.5080000162124634\n",
      "val_loss: 0.1098671555519104, ATE: 0.17707887291908264, ATE_hat: 0.10358493775129318, loss: 0.005401358474045992, balance: 0.5199999809265137\n",
      "val_loss: 0.0880010575056076, ATE: 0.09988224506378174, ATE_hat: 0.10815858095884323, loss: 6.849773490102962e-05, balance: 0.47200000286102295\n",
      "val_loss: 0.06488069146871567, ATE: 0.2041589915752411, ATE_hat: 0.07174573093652725, loss: 0.017533274367451668, balance: 0.5479999780654907\n",
      "val_loss: 0.0543658547103405, ATE: 0.18200461566448212, ATE_hat: 0.07882366329431534, loss: 0.010646308772265911, balance: 0.6000000238418579\n",
      "val_loss: 0.04484355077147484, ATE: 0.16941970586776733, ATE_hat: 0.11423324048519135, loss: 0.0030455458909273148, balance: 0.4480000138282776\n",
      "val_loss: 0.04153968766331673, ATE: 0.10765653848648071, ATE_hat: 0.09869912266731262, loss: 8.023529517231509e-05, balance: 0.4399999976158142\n",
      "val_loss: 0.02945583313703537, ATE: 0.2043703943490982, ATE_hat: 0.10404506325721741, loss: 0.010065171867609024, balance: 0.527999997138977\n",
      "val_loss: 0.0281794685870409, ATE: 0.09337913990020752, ATE_hat: 0.10030724853277206, loss: 4.7998688387451693e-05, balance: 0.492000013589859\n",
      "val_loss: 0.02242744155228138, ATE: 0.14229196310043335, ATE_hat: 0.11527731269598007, loss: 0.0007297913543879986, balance: 0.5080000162124634\n",
      "val_loss: 0.01847750134766102, ATE: 0.07980269193649292, ATE_hat: 0.10323206335306168, loss: 0.0005489354371093214, balance: 0.527999997138977\n",
      "val_loss: 0.01409926638007164, ATE: 0.09703031182289124, ATE_hat: 0.09400832653045654, loss: 9.132395462074783e-06, balance: 0.5479999780654907\n",
      "val_loss: 0.01174852903932333, ATE: -0.05922088027000427, ATE_hat: 0.10316599905490875, loss: 0.026369499042630196, balance: 0.5120000243186951\n",
      "val_loss: 0.01173726562410593, ATE: 0.0680253803730011, ATE_hat: 0.10884629935026169, loss: 0.001666347379796207, balance: 0.5360000133514404\n",
      "val_loss: 0.010341199114918709, ATE: 0.03416275978088379, ATE_hat: 0.09144479036331177, loss: 0.003281231038272381, balance: 0.5080000162124634\n",
      "val_loss: 0.010717005468904972, ATE: 0.051667213439941406, ATE_hat: 0.09371138364076614, loss: 0.001767712295986712, balance: 0.5479999780654907\n",
      "val_loss: 0.010944591835141182, ATE: 0.08366316556930542, ATE_hat: 0.09803073853254318, loss: 0.00020642715389840305, balance: 0.5600000023841858\n",
      "val_loss: 0.00935482420027256, ATE: 0.16705241799354553, ATE_hat: 0.09155584126710892, loss: 0.005699733272194862, balance: 0.5199999809265137\n",
      "val_loss: 0.007687239442020655, ATE: -0.04997794330120087, ATE_hat: 0.09148932993412018, loss: 0.0200129896402359, balance: 0.4480000138282776\n",
      "val_loss: 0.007927987724542618, ATE: 0.1025974303483963, ATE_hat: 0.09987901151180267, loss: 7.389800884993747e-06, balance: 0.515999972820282\n",
      "val_loss: 0.010331070981919765, ATE: 0.05464249849319458, ATE_hat: 0.10000545531511307, loss: 0.0020577977411448956, balance: 0.4880000054836273\n",
      "val_loss: 0.0087003568187356, ATE: 0.04102116823196411, ATE_hat: 0.10825453698635101, loss: 0.004520325921475887, balance: 0.5120000243186951\n",
      "val_loss: 0.008523979224264622, ATE: 0.07288596034049988, ATE_hat: 0.0954783484339714, loss: 0.0005104159936308861, balance: 0.5040000081062317\n",
      "val_loss: 0.008143296465277672, ATE: 0.08741803467273712, ATE_hat: 0.09608244150876999, loss: 7.50719482311979e-05, balance: 0.5\n",
      "val_loss: 0.006875393912196159, ATE: 0.06577908992767334, ATE_hat: 0.09829455614089966, loss: 0.0010572555474936962, balance: 0.4560000002384186\n",
      "val_loss: 0.006429586093872786, ATE: 0.19401568174362183, ATE_hat: 0.09152413159608841, loss: 0.010504517704248428, balance: 0.4880000054836273\n",
      "val_loss: 0.005157214589416981, ATE: 0.11649948358535767, ATE_hat: 0.09763374924659729, loss: 0.00035591592313721776, balance: 0.4880000054836273\n",
      "val_loss: 0.0041552879847586155, ATE: 0.1284784972667694, ATE_hat: 0.09660972654819489, loss: 0.0010156185599043965, balance: 0.4480000138282776\n",
      "val_loss: 0.0037522227503359318, ATE: 0.03926825523376465, ATE_hat: 0.10033155977725983, loss: 0.003728727111592889, balance: 0.5239999890327454\n",
      "val_loss: 0.004306152928620577, ATE: -0.0265941321849823, ATE_hat: 0.09734199941158295, loss: 0.01536016445606947, balance: 0.5\n",
      "val_loss: 0.00593747990205884, ATE: 0.12604135274887085, ATE_hat: 0.09686754643917084, loss: 0.0008511109626851976, balance: 0.4399999976158142\n",
      "val_loss: 0.006946386303752661, ATE: -0.0029497742652893066, ATE_hat: 0.0981694683432579, loss: 0.010225101374089718, balance: 0.4880000054836273\n",
      "val_loss: 0.007092930376529694, ATE: 0.1350795030593872, ATE_hat: 0.09156651049852371, loss: 0.0018933805404230952, balance: 0.5759999752044678\n",
      "val_loss: 0.006234295666217804, ATE: 0.1709543615579605, ATE_hat: 0.10168112069368362, loss: 0.004798782058060169, balance: 0.46799999475479126\n"
     ]
    }
   ],
   "source": [
    "cnn1.train(data_loader, epochs=200, validation_data = {\"X\":torch.tensor(X_val).float(), \n",
    "                                              \"tau\":torch.Tensor(tau_val).float()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn1.val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks without hidden layer, we can compare the coefficients to the known true coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ATE is usually approximately correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09801189472366993"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09859589], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn1.net.parameters())[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated coefficients often don't capture the direction and even approximate size of the effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00307794, -0.01166978, -0.00385822, -0.00118968, -0.00839613,\n",
       "       -0.01482367,  0.0107416 , -0.01228733, -0.00672245, -0.00236073])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03445683, -0.02473592, -0.02038331,  0.00429013, -0.02890932,\n",
       "        -0.02791671,  0.02570495,  0.00292601,  0.02517804, -0.00177828]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn1.net.parameters())[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted treatment effects are usually not a accurate estimate of the true individual treatment effects, but correlation is at least positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn1(torch.tensor(X).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09786279375105283 0.027739007909583484\n",
      "0.0983696 0.070951745\n",
      "[[1.         0.48864703]\n",
      " [0.48864703 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tau.mean(), tau.std())\n",
    "print(pred.mean(), pred.std())\n",
    "print(np.corrcoef(tau,pred.flatten(), rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08988072,  0.01105264,  0.10028923,  0.1462851 ,  0.09939626,\n",
       "        0.13995215,  0.10327283,  0.31601238, -0.00289319,  0.07911087],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07974348, 0.08498332, 0.10089869, 0.08148552, 0.06959321,\n",
       "       0.13259103, 0.1152406 , 0.12448763, 0.05613586, 0.07686927])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addditive two-model approach / residual network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Again estimate two networks jointly. Decompose the response estimate for treated observations into the response without treatment and the treatment effect. \n",
    "\n",
    "$$ \\hat{y}_t = f_R(X_t) + f_T(X_t) \\\\\n",
    "   \\hat{y}_t = f_R(X_c)$$\n",
    "\n",
    "One network $f_R$ predicts the response without treatment for each customer (treatment and control), the second network $f_T$ estimates the treatment effect. The loss for the first network is the response prediction MSE over all observations $$\\frac{1}{N}\\sum([\\hat{y_t};\\hat{y_c}] - [y_t;y_c])^2$$ the loss for the second network is the response prediction MSE for *only the treated group*, i.e. $$\\frac{1}{N_T}\\sum(\\hat{y_t} - y_t)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: This seems related to an approach called *covariate transformation* in the uplift literature. For covariate transformation, we estimate the outcome $y_i$ on a set of variables $[X_i, t_i \\cdot X_i]$, where $t_i \\cdot X_i$ is the interaction between the treatment indicator $t_i \\in {0;1}$ and observed variables $X$. \n",
    "\n",
    "TODO: For the linear regression model, this is equivalent to including each variable-treatment interaction term (?)\n",
    "\n",
    "TODO: I'm pretty sure that this works only when the response network $f_R$ is good. Check this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalnet2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.net0 = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.net1 = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net1(X)\n",
    "    \n",
    "    def train(self, data_loader, epochs, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            ATE, ATE_hat, loss, g_ratio = self._train_iteration(data_loader)\n",
    "            self.loss.append(loss)\n",
    "            if epoch % 5 == 0:\n",
    "                if validation_data is not None:\n",
    "                    tau_hat = self.net1(validation_data['X'])\n",
    "                    val_loss = (validation_data['tau'] - tau_hat).pow(2).mean().detach().numpy()\n",
    "                    self.val_loss.append(val_loss)\n",
    "                    print(f\"val_loss: {val_loss}, ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "                else:\n",
    "                    print(f\"ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "            \n",
    "    \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X0 = Variable(X[g==0,:])\n",
    "            y0 = Variable(y[g==0])\n",
    "            \n",
    "            X1 = Variable(X[g==1,:])\n",
    "            y1 = Variable(y[g==1])\n",
    "            \n",
    "            g_ratio = g.float().mean()\n",
    "            \n",
    "            response0 = self.net0(X0)\n",
    "            loss0 = ((y0 - response0).pow(2)).mean()\n",
    "            \n",
    "            response1 = self.net0(X1) + self.net1(X1)\n",
    "            loss1 = ((y1 - response1).pow(2)).mean()\n",
    "            \n",
    "            loss = loss0 + loss1\n",
    "            \n",
    "            ATE_hat = response1.mean() - response0.mean()\n",
    "            ATE     = y1.mean() - y0.mean()\n",
    "            loss_treatment = (ATE - ATE_hat).pow(2)            \n",
    "            \n",
    "            self.net0.zero_grad()\n",
    "            loss0.backward()\n",
    "            optim0.step()\n",
    "\n",
    "            self.net1.zero_grad()\n",
    "            loss1.backward()\n",
    "            optim1.step()\n",
    "                \n",
    "            \n",
    "        return ATE.detach().numpy(), ATE_hat.detach().numpy(), loss.detach().numpy(), g_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a FFNN without hidden layers as equivalent to a linear regression model. Note that the true model for y is non-linear, while the true model for the treatment effect is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = causalnet2(10, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim0 = Adam(cnn2.net0.parameters(), lr=0.001)\n",
    "optim1 = Adam(cnn2.net1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.07902302592992783, ATE: 0.11625814437866211, ATE_hat: -0.035088252276182175, loss: 1.0581822395324707, balance: 0.5438596606254578\n",
      "val_loss: 0.018727239221334457, ATE: 0.06554797291755676, ATE_hat: 0.19432908296585083, loss: 0.3741544485092163, balance: 0.48245614767074585\n",
      "val_loss: 0.0039681741036474705, ATE: 0.23010030388832092, ATE_hat: 0.1751628816127777, loss: 0.4842052459716797, balance: 0.42105263471603394\n",
      "val_loss: 0.003081697039306164, ATE: 0.1037260890007019, ATE_hat: 0.09680069983005524, loss: 0.3687504827976227, balance: 0.5175438523292542\n",
      "val_loss: 0.003194720484316349, ATE: 0.0292435884475708, ATE_hat: 0.14252597093582153, loss: 0.48138993978500366, balance: 0.5175438523292542\n",
      "val_loss: 0.0029647191986441612, ATE: 0.033151835203170776, ATE_hat: 0.09790477156639099, loss: 0.37130486965179443, balance: 0.5\n",
      "val_loss: 0.0025073355063796043, ATE: 0.1367647498846054, ATE_hat: 0.05682149529457092, loss: 0.36271199584007263, balance: 0.5263158082962036\n",
      "val_loss: 0.002758691320195794, ATE: 0.09732058644294739, ATE_hat: 0.08788073062896729, loss: 0.3541632890701294, balance: 0.42105263471603394\n",
      "val_loss: 0.003309920197352767, ATE: 0.2050780951976776, ATE_hat: 0.08298000693321228, loss: 0.4728640913963318, balance: 0.4736842215061188\n",
      "val_loss: 0.0033868332393467426, ATE: 0.16476772725582123, ATE_hat: 0.09351631999015808, loss: 0.37722674012184143, balance: 0.42105263471603394\n",
      "val_loss: 0.0030140739399939775, ATE: 0.16884759068489075, ATE_hat: 0.11120826005935669, loss: 0.3607179522514343, balance: 0.4649122953414917\n",
      "val_loss: 0.0030625450890511274, ATE: 0.09348171949386597, ATE_hat: 0.10205078125, loss: 0.298186331987381, balance: 0.5087719559669495\n",
      "val_loss: 0.0030637739691883326, ATE: 0.07284075021743774, ATE_hat: 0.06382009387016296, loss: 0.3951941430568695, balance: 0.42105263471603394\n",
      "val_loss: 0.0035011444706469774, ATE: 0.19243192672729492, ATE_hat: 0.13566994667053223, loss: 0.406033992767334, balance: 0.429824560880661\n",
      "val_loss: 0.002775871194899082, ATE: 0.08843293786048889, ATE_hat: 0.057966142892837524, loss: 0.40658634901046753, balance: 0.4649122953414917\n",
      "val_loss: 0.002775466302409768, ATE: 0.16614596545696259, ATE_hat: 0.147281676530838, loss: 0.49050819873809814, balance: 0.4912280738353729\n",
      "val_loss: 0.0031720860861241817, ATE: 0.07734847068786621, ATE_hat: 0.055847883224487305, loss: 0.3259674906730652, balance: 0.44736841320991516\n",
      "val_loss: 0.0026747456286102533, ATE: 0.06432217359542847, ATE_hat: 0.09867486357688904, loss: 0.6494370102882385, balance: 0.5438596606254578\n",
      "val_loss: 0.0029357285238802433, ATE: 0.08625540137290955, ATE_hat: -0.01875358819961548, loss: 0.3505702018737793, balance: 0.4649122953414917\n",
      "val_loss: 0.0028858475852757692, ATE: -0.05382883548736572, ATE_hat: 0.12964904308319092, loss: 0.37403714656829834, balance: 0.48245614767074585\n",
      "val_loss: 0.002957702148705721, ATE: 0.13924601674079895, ATE_hat: 0.11158260703086853, loss: 0.30172622203826904, balance: 0.5\n",
      "val_loss: 0.0026482839602977037, ATE: 0.0477062463760376, ATE_hat: 0.060521453619003296, loss: 0.2839146852493286, balance: 0.5438596606254578\n",
      "val_loss: 0.003241979517042637, ATE: 0.13152995705604553, ATE_hat: 0.05207699537277222, loss: 0.3803829550743103, balance: 0.5\n",
      "val_loss: 0.002959253266453743, ATE: 0.015596151351928711, ATE_hat: 0.08766880631446838, loss: 0.2875649929046631, balance: 0.5438596606254578\n",
      "val_loss: 0.002586161717772484, ATE: 0.07190737128257751, ATE_hat: 0.0691976547241211, loss: 0.38565945625305176, balance: 0.5\n",
      "val_loss: 0.002855616854503751, ATE: -0.08109113574028015, ATE_hat: 0.019008666276931763, loss: 0.41788530349731445, balance: 0.4912280738353729\n",
      "val_loss: 0.003128127893432975, ATE: -0.0359010249376297, ATE_hat: 0.08597522974014282, loss: 0.3584727644920349, balance: 0.42105263471603394\n",
      "val_loss: 0.0030465812887996435, ATE: 0.17463865876197815, ATE_hat: 0.11103641986846924, loss: 0.40067312121391296, balance: 0.5614035129547119\n",
      "val_loss: 0.0027800670359283686, ATE: 0.23294813930988312, ATE_hat: 0.0916329026222229, loss: 0.40006059408187866, balance: 0.5087719559669495\n",
      "val_loss: 0.0029109835159033537, ATE: 0.1977861076593399, ATE_hat: 0.06276339292526245, loss: 0.3836837112903595, balance: 0.5\n",
      "val_loss: 0.002909675007686019, ATE: 0.18197277188301086, ATE_hat: 0.16778425872325897, loss: 0.40273502469062805, balance: 0.5263158082962036\n",
      "val_loss: 0.0032099802047014236, ATE: 0.03677752614021301, ATE_hat: 0.08830386400222778, loss: 0.2686580717563629, balance: 0.4912280738353729\n",
      "val_loss: 0.003043362172320485, ATE: 0.11852839589118958, ATE_hat: 0.06262686848640442, loss: 0.3624700903892517, balance: 0.5263158082962036\n",
      "val_loss: 0.0028532652650028467, ATE: 0.06150926649570465, ATE_hat: 0.11919309198856354, loss: 0.3646125793457031, balance: 0.5263158082962036\n",
      "val_loss: 0.0027908182237297297, ATE: 0.1105780303478241, ATE_hat: 0.07507088780403137, loss: 0.42262113094329834, balance: 0.5438596606254578\n",
      "val_loss: 0.003257029689848423, ATE: 0.061341702938079834, ATE_hat: 0.06422185897827148, loss: 0.3057342767715454, balance: 0.4649122953414917\n",
      "val_loss: 0.0031096921302378178, ATE: 0.1340412199497223, ATE_hat: 0.114458829164505, loss: 0.33676785230636597, balance: 0.5087719559669495\n",
      "val_loss: 0.00282606715336442, ATE: 0.06527000665664673, ATE_hat: 0.06556817889213562, loss: 0.29465344548225403, balance: 0.5175438523292542\n",
      "val_loss: 0.0029766380321234465, ATE: 0.27794772386550903, ATE_hat: 0.11497867107391357, loss: 0.4190037250518799, balance: 0.41228070855140686\n",
      "val_loss: 0.0034844393376260996, ATE: 0.02420908212661743, ATE_hat: 0.11891481280326843, loss: 0.3924705982208252, balance: 0.4912280738353729\n"
     ]
    }
   ],
   "source": [
    "cnn2.train(data_loader, epochs=200, validation_data = {\"X\":torch.tensor(X_val).float(), \n",
    "                                              \"tau\":torch.Tensor(tau_val).float()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn2.val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks without hidden layer, we can compare the coefficients to the known true coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09801189472366993"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0971529], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn2.net1.parameters())[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated coefficients usually capture the direction and even approximate size of the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00307794, -0.01166978, -0.00385822, -0.00118968, -0.00839613,\n",
       "       -0.01482367,  0.0107416 , -0.01228733, -0.00672245, -0.00236073])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00299329, -0.01730319, -0.00615218,  0.00632256, -0.01877621,\n",
       "        -0.01581331,  0.01221997, -0.0279921 ,  0.00436302,  0.00056852]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn2.net1.parameters())[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted treatment effects are usually not a accurate estimate of the true individual treatment effects, but correlation is often high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn2.net1(torch.tensor(X).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09786279375105283 0.027739007909583484\n",
      "0.097075194 0.044248715\n",
      "[[1.        0.8558968]\n",
      " [0.8558968 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "print(tau.mean(), tau.std())\n",
    "print(pred.mean(), pred.std())\n",
    "print(np.corrcoef(tau,pred.flatten(), rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05481749, 0.08407508, 0.09644684, 0.10432093, 0.04511233,\n",
       "       0.17318489, 0.14628927, 0.16647863, 0.03747349, 0.06683242],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07974348, 0.08498332, 0.10089869, 0.08148552, 0.06959321,\n",
       "       0.13259103, 0.1152406 , 0.12448763, 0.05613586, 0.07686927])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
