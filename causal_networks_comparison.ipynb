{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generating process works as follows (story just for reference):\n",
    "Revenue ($y_i$) depends on some characteristics $X_i$ of the customer i. Customers are given a coupon (treatment $g_i$) with 50% probability . Customer who receive a coupon will spend more or less money (treatment effect $\\tau_i$) depending linearly on their characteristics.\n",
    "\n",
    "$$y_i = X_i ^\\top \\beta_X + g \\cdot X_i ^\\top \\beta_{\\tau}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment(n_obs,n_var, tau=None):\n",
    "    X = np.random.multivariate_normal(\n",
    "             np.zeros(n_var),\n",
    "             np.eye(n_var),\n",
    "             n_obs\n",
    "             )\n",
    "    \n",
    "    \n",
    "    beta = np.random.normal(scale=1, size=n_var)\n",
    "    beta_X2 = np.random.normal(scale=1, size=n_var)\n",
    "    beta_tau = np.random.normal(scale=0.1, size=n_var)\n",
    "    \n",
    "    g = np.hstack([np.ones(n_obs//2), np.zeros(n_obs//2)])\n",
    "        #np.random.binomial(1,0.5,size=n_obs)\n",
    "        \n",
    "    if tau is None:\n",
    "        tau = np.dot(X,beta_tau) #+ np.random.normal(scale=0.01, size=n_obs)\n",
    "        \n",
    "    y = np.dot(X,beta)  + tau*g            + np.random.normal(scale=0.1, size=n_obs)\n",
    "    #y = np.dot(X,beta) + np.dot(np.power(X,2),beta_X2) + tau*g + np.random.normal(scale=0.1, size=n_obs)\n",
    "    \n",
    "    return X, y, g, tau, beta, beta_X2, beta_tau\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentData(Dataset):\n",
    "    def __init__(self, X, y, g):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.g = g\n",
    "        \n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return X[idx,:], y[idx], g[idx]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a standard multi-layer perceptron. Classification only requires a change to the output activation from None to sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super(nnet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.layer_sizes = hidden_layer_sizes\n",
    "        self.iter = 0\n",
    "        \n",
    "        hidden_layer_sizes = hidden_layer_sizes + [1] # Output layer\n",
    "        first_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n",
    "        self.layers = nn.ModuleList(\n",
    "            [first_layer] +\\\n",
    "            [nn.Linear(input_, output_)\n",
    "             for input_, output_ in \n",
    "             zip(hidden_layer_sizes, hidden_layer_sizes[1:])])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, data_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self._train_iteration(data_loader)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"loss: {loss}\")\n",
    "                \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X = Variable(X, requires_grad=True)\n",
    "            y = Variable(y, requires_grad=True)\n",
    "                      \n",
    "            pred = self(X)\n",
    "            loss = ((y - pred)**2).mean()\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "               \n",
    "        return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch ATE causal net\n",
    "The two-model approach estimates two distinct models on the response variable, one for each group. The difference between the two estimates for the same observations is the treatment effect for a single observation. Disjoint estimation may result in models that are not well calibrated.\n",
    "\n",
    "Solution: Train a neural network to estimate the treatment effect directly. This is not possible for a single observation (->fundamental problem of causal inference):\n",
    "\n",
    "$$ r_i = \\hat{\\tau}_i - \\tau_i, $$\n",
    "where $\\tau_i$ is of course unknown. \n",
    "\n",
    "We can however evaluate the total error for groups of observations $i \\in 1,\\ldots,N$:\n",
    "\n",
    "$$ \\sum^N r_i = \\sum^N \\hat{\\tau}_i - \\tau_i = \\sum^N \\hat{\\tau}_i - \\sum^N \\tau_i$$\n",
    "\n",
    "\n",
    "The sum treatment effect is only weakly informative for the treatment effect of a single observation and estimating the overall sum of treatment effects leaves too many degrees of freedom for the treatment effect of each observations. By using mini-batches instead, the summed individual treatment effects need to be correct not only for the population N, but also for each subset of the population $M \\in N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalnet1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.net = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        treatment_effect = self.net(X)\n",
    "        return treatment_effect\n",
    "    \n",
    "    def train(self, data_loader, epochs, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            ATE, ATE_hat, loss, g_ratio = self._train_iteration(data_loader)\n",
    "            self.loss.append(loss)\n",
    "            if epoch % 5 == 0:\n",
    "                if validation_data is not None:\n",
    "                    tau_hat = self.net(validation_data['X'])\n",
    "                    val_loss = (validation_data['tau'] - tau_hat).pow(2).mean().detach().numpy()\n",
    "                    self.val_loss.append(val_loss)\n",
    "                    print(f\"val_loss: {val_loss}, ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "                else:\n",
    "                    print(f\"ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "            \n",
    "    \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X0 = Variable(X[g==0,:])\n",
    "            y0 = Variable(y[g==0])\n",
    "            \n",
    "            X1 = Variable(X[g==1,:])\n",
    "            y1 = Variable(y[g==1])\n",
    "            \n",
    "            g_ratio = g.float().mean()\n",
    "            \n",
    "#             response0 = self.net0(X0)\n",
    "#             loss0 = ((y0 - response0)**2).mean()\n",
    "            \n",
    "#             response1 = self.net1(X1)\n",
    "#             loss1 = ((y1 - response1)**2).mean()\n",
    "\n",
    "            treatment_effect = self.net(X)\n",
    "            \n",
    "            ATE_hat = treatment_effect.mean()\n",
    "            ATE     = y1.mean() - y0.mean()\n",
    "            loss_treatment = (ATE - ATE_hat)**2            \n",
    "            \n",
    "            \n",
    "#             if i % 2==0:\n",
    "#                 self.net0.zero_grad()\n",
    "#                 loss =  0*loss0 + loss_treatment\n",
    "#                 loss.backward()\n",
    "#                 optim0.step()\n",
    "#             else:\n",
    "            self.net.zero_grad()\n",
    "            loss =  loss_treatment\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "                \n",
    "            \n",
    "        return ATE.detach().numpy(), ATE_hat.detach().numpy(), loss.detach().numpy(), g_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addditive two-model approach / residual network\n",
    "Again estimate two networks jointly. Decompose the response estimate for treated observations into the response without treatment and the treatment effect. \n",
    "\n",
    "$$ \\hat{y}_t = f_R(X_t) + f_T(X_t) \\\\\n",
    "   \\hat{y}_t = f_R(X_c)$$\n",
    "\n",
    "One network $f_R$ predicts the response without treatment for each customer (treatment and control), the second network $f_T$ estimates the treatment effect. The loss for the first network is the response prediction MSE over all observations $$\\frac{1}{N}\\sum([\\hat{y_t};\\hat{y_c}] - [y_t;y_c])^2$$ the loss for the second network is the response prediction MSE for *only the treated group*, i.e. $$\\frac{1}{N_T}\\sum(\\hat{y_t} - y_t)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems related to an approach called *covariate transformation* in the uplift literature. For covariate transformation, we estimate the outcome $y_i$ on a set of variables $[X_i, t_i \\cdot X_i]$, where $t_i \\cdot X_i$ is the interaction between the treatment indicator $t_i \\in {0;1}$ and observed variables $X$. \n",
    "\n",
    "For the linear regression model, this is equivalent to including each variable-treatment interaction term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalnet2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.net0 = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.net1 = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.loss = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net1(X)\n",
    "    \n",
    "    def train(self, data_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            ATE_diff, response_loss, g_ratio = self._train_iteration(data_loader)\n",
    "            self.loss.append(response_loss)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"ATE Difference: {ATE_diff}, Response MSE: {response_loss}, balance: {g_ratio}\")\n",
    "            \n",
    "    \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X0 = Variable(X[g==0,:])\n",
    "            y0 = Variable(y[g==0])\n",
    "            \n",
    "            X1 = Variable(X[g==1,:])\n",
    "            y1 = Variable(y[g==1])\n",
    "            \n",
    "            g_ratio = g.float().mean()\n",
    "            \n",
    "            response0 = self.net0(X0)\n",
    "            loss0 = ((y0 - response0).pow(2)).mean()\n",
    "            \n",
    "            response1 = self.net0(X1) + self.net1(X1)\n",
    "            loss1 = ((y1 - response1).pow(2)).mean()\n",
    "            \n",
    "            ATE_hat = response1.mean() - response0.mean()\n",
    "            ATE     = y1.mean() - y0.mean()\n",
    "            loss_treatment = (ATE - ATE_hat).pow(2)            \n",
    "            \n",
    "            self.net0.zero_grad()\n",
    "            loss0.backward()\n",
    "            optim0.step()\n",
    "\n",
    "            if i > 10:\n",
    "                self.net1.zero_grad()\n",
    "                loss1.backward()\n",
    "                optim1.step()\n",
    "                \n",
    "            \n",
    "        return ATE.detach().numpy()-ATE_hat.detach().numpy(), loss0.detach().numpy(), g_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, g, tau, coef, coef_x2, coef_tau = generate_experiment(15000,10, tau=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation in the simulation context, I assume that the treatment effects are known and calculate the accuracy on the model in estimating the treatment effects on a holdout validation set. In practice, the true treatment effects are unknown, of course, so holdout evaluation is an open question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_val, y, y_val, g, g_val, tau, tau_val = train_test_split(X,y,g,tau, stratify=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ExperimentData(X,y,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: A dynamic decrease in batch size could provide more and more information given that the model is stable enough to create decent estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caveat: Ideally, the data loader will pass bs/2 observations of each the treatment and the control group, but this will take some coding, so I ignore it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0014983127474007428, 0.29189150511155104)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True ATE and standard deviation of individual treatment effects\n",
    "np.mean(tau), np.std(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07037659919797251"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empirical ATE\n",
    "np.mean(y[g==1]) - np.mean(y[g==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = causalnet1(10, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low learning rate is possibly necessary to stabilize training given the noise in the ATE within each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(cnn.net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim0 = Adam(cnn.net0.parameters(), lr=0.001)\n",
    "# optim1 = Adam(cnn.net1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.5005821585655212, ATE: 0.6170765161514282, ATE_hat: -0.20471200346946716, loss: 0.6753364205360413, balance: 0.4716981053352356\n",
      "val_loss: 0.46363651752471924, ATE: -1.2764742374420166, ATE_hat: 0.042919158935546875, loss: 1.7407989501953125, balance: 0.4716981053352356\n",
      "val_loss: 0.4214998781681061, ATE: 1.1167219877243042, ATE_hat: 0.041933588683605194, loss: 1.155170202255249, balance: 0.5471698045730591\n",
      "val_loss: 0.4852908253669739, ATE: -2.7892675399780273, ATE_hat: -0.020373757928609848, loss: 7.666772365570068, balance: 0.4716981053352356\n",
      "val_loss: 0.4960891306400299, ATE: 2.2309157848358154, ATE_hat: 0.07427241653203964, loss: 4.651110649108887, balance: 0.4528301954269409\n",
      "val_loss: 0.4961881637573242, ATE: -1.569047451019287, ATE_hat: 0.14438094198703766, loss: 2.9358367919921875, balance: 0.4528301954269409\n",
      "val_loss: 0.4388801157474518, ATE: -0.0762040913105011, ATE_hat: 0.035463374108076096, loss: 0.012469623237848282, balance: 0.5849056839942932\n",
      "val_loss: 0.3868575990200043, ATE: 2.653785467147827, ATE_hat: 0.11497366428375244, loss: 6.4455647468566895, balance: 0.43396225571632385\n",
      "val_loss: 0.36424583196640015, ATE: -0.9093626737594604, ATE_hat: 0.0968669131398201, loss: 1.012498140335083, balance: 0.49056604504585266\n",
      "val_loss: 0.3384259045124054, ATE: -0.5998514890670776, ATE_hat: -0.08301040530204773, loss: 0.2671246826648712, balance: 0.5094339847564697\n"
     ]
    }
   ],
   "source": [
    "cnn.train(data_loader,50, validation_data = {\"X\":torch.tensor(X_val).float(), \n",
    "                                              \"tau\":torch.Tensor(tau_val).float()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn.val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks without hidden layer, we can compare the coefficients to the known true coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0841725 ,  0.09875353,  0.07142214,  0.04690281, -0.11582379,\n",
       "        -0.10721895, -0.00215474, -0.00468169,  0.06340275,  0.19547875]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn.net.parameters())[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04881745,  0.13788995,  0.05775231,  0.07128759, -0.0433989 ,\n",
       "       -0.09319255, -0.04073546, -0.01610772,  0.16573541,  0.122049  ])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated coefficients don't seem to fit very well, usually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn(torch.tensor(X).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.061663933"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02150818, -0.5197296 ,  0.05525255, -0.14395401, -0.03769255,\n",
       "        0.6021647 , -0.35010713,  0.58953685,  0.11604905,  0.12150869],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03013892, -0.0843743 ,  0.18227094, -0.26273309,  0.78784508,\n",
       "        0.31387099, -0.1277751 ,  0.27967361,  0.08167208, -0.07879074])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
