{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a standard multi-layer perceptron. Classification only requires a change to the output activation from None to sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super(nnet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.layer_sizes = hidden_layer_sizes\n",
    "        self.iter = 0\n",
    "        \n",
    "        hidden_layer_sizes = hidden_layer_sizes + [1] # Output layer\n",
    "        first_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n",
    "        self.layers = nn.ModuleList(\n",
    "            [first_layer] +\\\n",
    "            [nn.Linear(input_, output_)\n",
    "             for input_, output_ in \n",
    "             zip(hidden_layer_sizes, hidden_layer_sizes[1:])])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, data_loader, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self._train_iteration(data_loader)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"loss: {loss}\")\n",
    "                \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X = Variable(X, requires_grad=True)\n",
    "            y = Variable(y, requires_grad=True)\n",
    "                      \n",
    "            pred = self(X)\n",
    "            loss = ((y - pred)**2).mean()\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "               \n",
    "        return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generating process works as follows (story just for reference):\n",
    "Revenue ($y_i$) depends on some characteristics $X_i$ of the customer i. Customers are given a coupon (treatment $g_i$) with 50% probability . Customer who receive a coupon will spend more or less money (treatment effect $\\tau_i$) depending linearly on their characteristics.\n",
    "\n",
    "$$y_i = X_i ^\\top \\beta_X + g_i \\cdot (\\tau_0 + X_i ^\\top \\beta_{\\tau} + \\epsilon^{\\tau}_i) + \\epsilon_i$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\\epsilon_i \\sim \\text{Normal}(mean = 0, std = 0.1)$$\n",
    "\n",
    "$$g_i \\sim \\text{Bernoulli}(p=0.5)$$\n",
    "$$\\epsilon^{\\tau}_i \\sim \\text{Normal}(mean = 0, std = 0.001)$$\n",
    "\n",
    "I think there is merit to the assumption that the true reponse model is often more complex than the model behind the heterogeneity of treatment effects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment(n_obs,n_var, tau=None):\n",
    "    X = np.random.multivariate_normal(\n",
    "             np.zeros(n_var),\n",
    "             np.eye(n_var),\n",
    "             n_obs\n",
    "             )\n",
    "    \n",
    "    \n",
    "    # Linear effects\n",
    "    beta     = np.random.normal(scale=0.1, size=n_var)\n",
    "    # Non-linear effects (optional)\n",
    "    beta_X2  = np.random.normal(loc=0, scale=0.1, size=n_var)\n",
    "    # Linear effects on treatment effect\n",
    "    beta_tau = np.random.normal(loc=0,scale=0.01, size=n_var)\n",
    "    # Baseline treatment effect\n",
    "    tau_zero = np.random.normal(0.1,0.01)\n",
    "    \n",
    "    g = np.hstack([np.ones(n_obs//2), np.zeros(n_obs//2)])\n",
    "        #np.random.binomial(1,0.5,size=n_obs)\n",
    "        \n",
    "    if tau is None:\n",
    "        tau = tau_zero + np.dot(X,beta_tau) + np.random.normal(scale=0.001, size=n_obs)\n",
    "        \n",
    "    y = np.dot(X,beta) +\\\n",
    "        np.dot(np.power(X,2),beta_X2) +\\\n",
    "        g * tau + np.random.normal(scale=0.1, size=n_obs)\n",
    "    \n",
    "    return X, y, g, tau, beta, beta_X2, tau_zero, beta_tau\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataloader is currently a random sampler (see below) that returns $X_i$, $y_i$ and $g_i$ for a batch. \n",
    "\n",
    "TODO: I think a batch sampler stratified on $g$ makes more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentData(Dataset):\n",
    "    def __init__(self, X, y, g):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.g = g\n",
    "        \n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return X[idx,:], y[idx], g[idx]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, g, tau, coef, coef_x2, tau_zero, coef_tau = generate_experiment(15000,10, tau=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation in the simulation context, I assume that the treatment effects are known and calculate the accuracy on the model in estimating the treatment effects on a holdout validation set. In practice, the true treatment effects are unknown, of course, so holdout evaluation is an open question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.30378227001716507, 0.38715602598402266)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATE summary statistics. These should be stable to confirm that the info-noise ratio in the data is not too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline treatment effect (True ATE):0.09941928491355384\n",
      "Sample treatment effect (ITE Mean, ITE Std.): (0.09968136775466813, 0.02549210466909146)\n",
      "Empirical ATE: 0.09203177699345721\n"
     ]
    }
   ],
   "source": [
    "# True ATE and standard deviation of individual treatment effects\n",
    "print(f\"Baseline treatment effect (True ATE):{tau_zero}\")\n",
    "print(f\"Sample treatment effect (ITE Mean, ITE Std.): {np.mean(tau), np.std(tau)}\")\n",
    "print(f\"Empirical ATE: {np.mean(y[g==1]) - np.mean(y[g==0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_val, y, y_val, g, g_val, tau, tau_val = train_test_split(X,y,g,tau, stratify=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE for treatment effect prediction on validation data:      0.25560378150483054\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline MSE for treatment effect prediction on validation data:\\\n",
    "      {np.mean((tau_val - np.mean(y[g==1]) - np.mean(y[g==0]))**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ExperimentData(X,y,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch ATE causal net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The two-model approach estimates two distinct models on the response variable, one for each group. The difference between the two estimates for the same observations is the treatment effect for a single observation. Disjoint estimation may result in models that are not well calibrated.\n",
    "\n",
    "Solution: Train a neural network to estimate the treatment effect directly. This is not possible for a single observation (-> fundamental problem of causal inference):\n",
    "\n",
    "$$ r_i = \\hat{\\tau}_i - \\tau_i, $$\n",
    "where $\\tau_i$ is of course unknown. \n",
    "\n",
    "We can however evaluate the total error for groups of observations $i \\in 1,\\ldots,N$:\n",
    "\n",
    "$$ \\sum^N r_i = \\sum^N \\hat{\\tau}_i - \\tau_i = \\sum^N \\hat{\\tau}_i - \\sum^N \\tau_i$$\n",
    "\n",
    "\n",
    "The sum treatment effect is only weakly informative for the treatment effect of a single observation and estimating the overall sum of treatment effects leaves too many degrees of freedom for the treatment effect of each observations. By using mini-batches instead, the summed individual treatment effects need to be correct not only for the population N, but also for each subset of the population $M \\in N$.\n",
    "\n",
    "The trick is in shuffling and reshuffling the observation between the batches. On the full sample, the model can get away with predicting the ATE for each observation. For the ATE *in each subset* to be correct, the model needs to predict the individual treatment effects correctly. \n",
    "\n",
    "TODO: Show this formally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalnet1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.net = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        treatment_effect = self.net(X)\n",
    "        return treatment_effect\n",
    "    \n",
    "    def train(self, data_loader, epochs, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            ATE, ATE_hat, loss, g_ratio = self._train_iteration(data_loader)\n",
    "            self.loss.append(loss)\n",
    "            if epoch % 5 == 0:\n",
    "                if validation_data is not None:\n",
    "                    tau_hat = self.net(validation_data['X'])\n",
    "                    val_loss = (validation_data['tau'] - tau_hat).pow(2).mean().detach().numpy()\n",
    "                    self.val_loss.append(val_loss)\n",
    "                    print(f\"val_loss: {val_loss}, ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "                else:\n",
    "                    print(f\"ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "            \n",
    "    \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X0 = Variable(X[g==0,:])\n",
    "            y0 = Variable(y[g==0])\n",
    "            \n",
    "            X1 = Variable(X[g==1,:])\n",
    "            y1 = Variable(y[g==1])\n",
    "            \n",
    "            g_ratio = g.float().mean()\n",
    "            \n",
    "#             response0 = self.net0(X0)\n",
    "#             loss0 = ((y0 - response0)**2).mean()\n",
    "            \n",
    "#             response1 = self.net1(X1)\n",
    "#             loss1 = ((y1 - response1)**2).mean()\n",
    "\n",
    "            treatment_effect = self.net(X)\n",
    "            \n",
    "            ATE_hat = treatment_effect.mean()\n",
    "            ATE     = y1.mean() - y0.mean()\n",
    "            loss_treatment = (ATE - ATE_hat)**2            \n",
    "            \n",
    "            \n",
    "#             if i % 2==0:\n",
    "#                 self.net0.zero_grad()\n",
    "#                 loss =  0*loss0 + loss_treatment\n",
    "#                 loss.backward()\n",
    "#                 optim0.step()\n",
    "#             else:\n",
    "            self.net.zero_grad()\n",
    "            loss =  loss_treatment\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "                \n",
    "            \n",
    "        return ATE.detach().numpy(), ATE_hat.detach().numpy(), loss.detach().numpy(), g_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My intuition about the batch size is tradeoff: Too large and the signal from each observation becomes too small and predicting the ATE is the dominant strategy. Too small and the training becomes unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: A dynamic decrease in batch size could provide more and more information given that the model is stable enough to create decent estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Ideally, the data loader will pass bs/2 observations of each the treatment and the control group, but this will take some coding, so I ignore it for now. For small batch sizes, it's currently possible that not both groups are present in the batch so the loss will return NaN and training fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a FFNN without hidden layers as equivalent to the true linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = causalnet1(10, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A low learning rate is possibly necessary to stabilize training given the noise in the ATE within each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(cnn1.net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim0 = Adam(cnn.net0.parameters(), lr=0.001)\n",
    "# optim1 = Adam(cnn.net1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.3939637541770935, ATE: 0.10460539162158966, ATE_hat: -0.15642894804477692, loss: 0.06813892722129822, balance: 0.4663366377353668\n",
      "val_loss: 0.3599683940410614, ATE: 0.09905397891998291, ATE_hat: -0.12370926141738892, loss: 0.049623459577560425, balance: 0.48712870478630066\n",
      "val_loss: 0.33551907539367676, ATE: 0.12718407809734344, ATE_hat: -0.07809216529130936, loss: 0.04213833808898926, balance: 0.5178217887878418\n",
      "val_loss: 0.32068872451782227, ATE: 0.09457835555076599, ATE_hat: -0.04236472770571709, loss: 0.018753409385681152, balance: 0.4821782112121582\n",
      "val_loss: 0.3120989501476288, ATE: 0.11626145243644714, ATE_hat: 0.01940097101032734, loss: 0.009381953626871109, balance: 0.512871265411377\n",
      "val_loss: 0.3062618672847748, ATE: 0.11410671472549438, ATE_hat: 0.02787178009748459, loss: 0.007436464074999094, balance: 0.4752475321292877\n",
      "val_loss: 0.29948824644088745, ATE: 0.09965014457702637, ATE_hat: 0.06370097398757935, loss: 0.001292342902161181, balance: 0.49702969193458557\n",
      "val_loss: 0.29192742705345154, ATE: 0.109487384557724, ATE_hat: 0.06496118009090424, loss: 0.0019825829658657312, balance: 0.48712870478630066\n",
      "val_loss: 0.28555163741111755, ATE: 0.09520623087882996, ATE_hat: 0.09307664632797241, loss: 4.53513030151953e-06, balance: 0.48712870478630066\n",
      "val_loss: 0.27885401248931885, ATE: 0.09982024133205414, ATE_hat: 0.07894858717918396, loss: 0.00043562595965340734, balance: 0.5326732397079468\n",
      "val_loss: 0.2697756886482239, ATE: 0.0860138088464737, ATE_hat: 0.10689328610897064, loss: 0.0004359525628387928, balance: 0.5059406161308289\n",
      "val_loss: 0.2634243071079254, ATE: 0.10074320435523987, ATE_hat: 0.06999925523996353, loss: 0.0009451904334127903, balance: 0.498019814491272\n",
      "val_loss: 0.25718340277671814, ATE: 0.07776293158531189, ATE_hat: 0.10250335931777954, loss: 0.0006120887701399624, balance: 0.5\n",
      "val_loss: 0.24682417511940002, ATE: 0.1278030127286911, ATE_hat: 0.09451033920049667, loss: 0.0011084021534770727, balance: 0.49405941367149353\n",
      "val_loss: 0.23561570048332214, ATE: 0.08400815725326538, ATE_hat: 0.10667934268712997, loss: 0.0005139826680533588, balance: 0.5\n",
      "val_loss: 0.22054064273834229, ATE: 0.07691913843154907, ATE_hat: 0.09576968848705292, loss: 0.0003553432470653206, balance: 0.5079208016395569\n",
      "val_loss: 0.21091006696224213, ATE: 0.12364666163921356, ATE_hat: 0.07803577184677124, loss: 0.002080353209748864, balance: 0.4732673168182373\n",
      "val_loss: 0.1987113058567047, ATE: 0.10863113403320312, ATE_hat: 0.10228323191404343, loss: 4.029586125398055e-05, balance: 0.49603959918022156\n",
      "val_loss: 0.19098520278930664, ATE: 0.1574845314025879, ATE_hat: 0.10541821271181107, loss: 0.0027109014336019754, balance: 0.49702969193458557\n",
      "val_loss: 0.1802978366613388, ATE: 0.09962081909179688, ATE_hat: 0.10455211997032166, loss: 2.4317729184986092e-05, balance: 0.5396039485931396\n",
      "val_loss: 0.1720374971628189, ATE: 0.1096058338880539, ATE_hat: 0.08072296530008316, loss: 0.0008342200890183449, balance: 0.5099009871482849\n",
      "val_loss: 0.1626783162355423, ATE: 0.12300980091094971, ATE_hat: 0.1016327440738678, loss: 0.00045697856694459915, balance: 0.513861358165741\n",
      "val_loss: 0.15610039234161377, ATE: 0.07614034414291382, ATE_hat: 0.08843313902616501, loss: 0.00015111280663404614, balance: 0.47623762488365173\n",
      "val_loss: 0.14637653529644012, ATE: 0.06476262211799622, ATE_hat: 0.08756976574659348, loss: 0.0005201657768338919, balance: 0.5059406161308289\n",
      "val_loss: 0.14081217348575592, ATE: 0.12451156973838806, ATE_hat: 0.08359695971012115, loss: 0.0016740052960813046, balance: 0.503960371017456\n",
      "val_loss: 0.1290147304534912, ATE: 0.09791702032089233, ATE_hat: 0.07792935520410538, loss: 0.0003995067672803998, balance: 0.5059406161308289\n",
      "val_loss: 0.12107009440660477, ATE: 0.10678732395172119, ATE_hat: 0.08443781733512878, loss: 0.0004995004273951054, balance: 0.499009907245636\n",
      "val_loss: 0.1173335388302803, ATE: 0.05211421847343445, ATE_hat: 0.0849122703075409, loss: 0.0010757121490314603, balance: 0.49504950642585754\n",
      "val_loss: 0.11391177773475647, ATE: 0.07181510329246521, ATE_hat: 0.09043285250663757, loss: 0.00034662059624679387, balance: 0.5108910799026489\n",
      "val_loss: 0.10845226794481277, ATE: 0.1356135904788971, ATE_hat: 0.0734553188085556, loss: 0.0038636508397758007, balance: 0.48613861203193665\n",
      "val_loss: 0.10099555552005768, ATE: 0.09326988458633423, ATE_hat: 0.09308914840221405, loss: 3.266556802827836e-08, balance: 0.5099009871482849\n",
      "val_loss: 0.09579352289438248, ATE: 0.09288620948791504, ATE_hat: 0.08457514643669128, loss: 6.907377246534452e-05, balance: 0.48712870478630066\n",
      "val_loss: 0.09106966108083725, ATE: 0.1352856159210205, ATE_hat: 0.09622896462678909, loss: 0.0015254219761118293, balance: 0.5118811726570129\n",
      "val_loss: 0.08611715584993362, ATE: 0.04913359880447388, ATE_hat: 0.08880436420440674, loss: 0.0015737696085125208, balance: 0.49603959918022156\n",
      "val_loss: 0.0813271701335907, ATE: 0.1395503282546997, ATE_hat: 0.10295595973730087, loss: 0.001339147798717022, balance: 0.48712870478630066\n",
      "val_loss: 0.07926081866025925, ATE: 0.10880807042121887, ATE_hat: 0.10227233171463013, loss: 4.2715881136246026e-05, balance: 0.4881187975406647\n",
      "val_loss: 0.07501266896724701, ATE: 0.10245952010154724, ATE_hat: 0.09482655674219131, loss: 5.8262128732167184e-05, balance: 0.5316831469535828\n",
      "val_loss: 0.06690536439418793, ATE: 0.07490098476409912, ATE_hat: 0.09430989623069763, loss: 0.0003767058369703591, balance: 0.502970278263092\n",
      "val_loss: 0.06379053741693497, ATE: 0.1088167130947113, ATE_hat: 0.10876422375440598, loss: 2.755130790177418e-09, balance: 0.499009907245636\n",
      "val_loss: 0.06058217212557793, ATE: 0.0743061900138855, ATE_hat: 0.08928580582141876, loss: 0.00022438888845499605, balance: 0.5148515105247498\n"
     ]
    }
   ],
   "source": [
    "cnn1.train(data_loader, epochs=200, validation_data = {\"X\":torch.tensor(X_val).float(), \n",
    "                                              \"tau\":torch.Tensor(tau_val).float()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn1.val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks without hidden layer, we can compare the coefficients to the known true coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ATE is usually approximately correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09941928491355384"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09392048], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn1.net.parameters())[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated coefficients often don't capture the direction and even approximate size of the effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01181628, -0.00056324, -0.00038409, -0.0026142 ,  0.00355688,\n",
       "       -0.01024895,  0.00623549,  0.00018308, -0.00534166, -0.01784753])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02235821, -0.09537394,  0.03654303,  0.05068643,  0.12570445,\n",
       "        -0.09733441,  0.04787605,  0.08505245, -0.03988213,  0.08098884]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn1.net.parameters())[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted treatment effects are usually not a accurate estimate of the true individual treatment effects, but correlation is at least positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn1(torch.tensor(X).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09954795307329109 0.02537136176155067\n",
      "0.09410434 0.23787782\n",
      "[[1.         0.02499012]\n",
      " [0.02499012 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tau.mean(), tau.std())\n",
    "print(pred.mean(), pred.std())\n",
    "print(np.corrcoef(tau,pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10420389,  0.11199708, -0.20473675,  0.02476408, -0.19007854,\n",
       "       -0.17463998,  0.21059147,  0.05530845,  0.02547589,  0.30920845],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12381572, 0.10763013, 0.04988213, 0.0977745 , 0.12228992,\n",
       "       0.13137319, 0.1038555 , 0.10988424, 0.09399612, 0.09210034])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addditive two-model approach / residual network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Again estimate two networks jointly. Decompose the response estimate for treated observations into the response without treatment and the treatment effect. \n",
    "\n",
    "$$ \\hat{y}_t = f_R(X_t) + f_T(X_t) \\\\\n",
    "   \\hat{y}_t = f_R(X_c)$$\n",
    "\n",
    "One network $f_R$ predicts the response without treatment for each customer (treatment and control), the second network $f_T$ estimates the treatment effect. The loss for the first network is the response prediction MSE over all observations $$\\frac{1}{N}\\sum([\\hat{y_t};\\hat{y_c}] - [y_t;y_c])^2$$ the loss for the second network is the response prediction MSE for *only the treated group*, i.e. $$\\frac{1}{N_T}\\sum(\\hat{y_t} - y_t)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: This seems related to an approach called *covariate transformation* in the uplift literature. For covariate transformation, we estimate the outcome $y_i$ on a set of variables $[X_i, t_i \\cdot X_i]$, where $t_i \\cdot X_i$ is the interaction between the treatment indicator $t_i \\in {0;1}$ and observed variables $X$. \n",
    "\n",
    "TODO: For the linear regression model, this is equivalent to including each variable-treatment interaction term (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causalnet2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes):\n",
    "        super().__init__()\n",
    "        self.net0 = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.net1 = nnet(input_dim, hidden_layer_sizes)\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.net1(X)\n",
    "    \n",
    "    def train(self, data_loader, epochs, validation_data=None):\n",
    "        for epoch in range(epochs):\n",
    "            ATE, ATE_hat, loss, g_ratio = self._train_iteration(data_loader)\n",
    "            self.loss.append(loss)\n",
    "            if epoch % 5 == 0:\n",
    "                if validation_data is not None:\n",
    "                    tau_hat = self.net1(validation_data['X'])\n",
    "                    val_loss = (validation_data['tau'] - tau_hat).pow(2).mean().detach().numpy()\n",
    "                    self.val_loss.append(val_loss)\n",
    "                    print(f\"val_loss: {val_loss}, ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "                else:\n",
    "                    print(f\"ATE: {ATE}, ATE_hat: {ATE_hat}, loss: {loss}, balance: {g_ratio}\")\n",
    "            \n",
    "    \n",
    "    def _train_iteration(self,data_loader):\n",
    "        for i, (X,y,g) in enumerate(data_loader):\n",
    "            \n",
    "            X = X.float()\n",
    "            y = y.unsqueeze(1).float()\n",
    "            \n",
    "            X0 = Variable(X[g==0,:])\n",
    "            y0 = Variable(y[g==0])\n",
    "            \n",
    "            X1 = Variable(X[g==1,:])\n",
    "            y1 = Variable(y[g==1])\n",
    "            \n",
    "            g_ratio = g.float().mean()\n",
    "            \n",
    "            response0 = self.net0(X0)\n",
    "            loss0 = ((y0 - response0).pow(2)).mean()\n",
    "            \n",
    "            response1 = self.net0(X1) + self.net1(X1)\n",
    "            loss1 = ((y1 - response1).pow(2)).mean()\n",
    "            \n",
    "            loss = loss0 + loss1\n",
    "            \n",
    "            ATE_hat = response1.mean() - response0.mean()\n",
    "            ATE     = y1.mean() - y0.mean()\n",
    "            loss_treatment = (ATE - ATE_hat).pow(2)            \n",
    "            \n",
    "            self.net0.zero_grad()\n",
    "            loss0.backward()\n",
    "            optim0.step()\n",
    "\n",
    "            self.net1.zero_grad()\n",
    "            loss1.backward()\n",
    "            optim1.step()\n",
    "                \n",
    "            \n",
    "        return ATE.detach().numpy(), ATE_hat.detach().numpy(), loss.detach().numpy(), g_ratio\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a FFNN without hidden layers as equivalent to a linear regression model. Note that the true model for y is non-linear, while the true model for the treatment effect is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = causalnet2(10, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim0 = Adam(cnn2.net0.parameters(), lr=0.001)\n",
    "optim1 = Adam(cnn2.net1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.31983739137649536, ATE: 0.039856791496276855, ATE_hat: 0.2881721258163452, loss: 0.7874999046325684, balance: 0.5087719559669495\n",
      "val_loss: 0.05237380415201187, ATE: 0.03538721799850464, ATE_hat: 0.12635758519172668, loss: 0.1336619108915329, balance: 0.5087719559669495\n",
      "val_loss: 0.0027038927655667067, ATE: 0.2698028087615967, ATE_hat: 0.2562141716480255, loss: 0.13276541233062744, balance: 0.4736842215061188\n",
      "val_loss: 0.0014801514334976673, ATE: 0.14661186933517456, ATE_hat: 0.14969894289970398, loss: 0.14468304812908173, balance: 0.5263158082962036\n",
      "val_loss: 0.001657499698922038, ATE: -0.004258185625076294, ATE_hat: 0.10067881643772125, loss: 0.13434848189353943, balance: 0.4649122953414917\n",
      "val_loss: 0.0015906670596450567, ATE: 0.022585421800613403, ATE_hat: -0.040996626019477844, loss: 0.15352189540863037, balance: 0.45614033937454224\n",
      "val_loss: 0.0016710431082174182, ATE: 0.1673550307750702, ATE_hat: 0.12963742017745972, loss: 0.1302785873413086, balance: 0.4912280738353729\n",
      "val_loss: 0.0016228032764047384, ATE: 0.13480795919895172, ATE_hat: 0.08972875773906708, loss: 0.13871365785598755, balance: 0.5\n",
      "val_loss: 0.001696393359452486, ATE: 0.18891632556915283, ATE_hat: 0.15686167776584625, loss: 0.13847407698631287, balance: 0.5789473652839661\n",
      "val_loss: 0.0015773770865052938, ATE: -0.037367552518844604, ATE_hat: 0.05558043718338013, loss: 0.10169339179992676, balance: 0.4912280738353729\n",
      "val_loss: 0.0018846419407054782, ATE: 0.1435239464044571, ATE_hat: 0.18675439059734344, loss: 0.14251112937927246, balance: 0.5614035129547119\n",
      "val_loss: 0.0015663305530324578, ATE: 0.1447698473930359, ATE_hat: 0.17102397978305817, loss: 0.17206044495105743, balance: 0.5614035129547119\n",
      "val_loss: 0.0017408381681889296, ATE: 0.16718734800815582, ATE_hat: 0.16545438766479492, loss: 0.102955162525177, balance: 0.4736842215061188\n",
      "val_loss: 0.0014190274523571134, ATE: 0.15850158035755157, ATE_hat: 0.12695877254009247, loss: 0.12274298071861267, balance: 0.5\n",
      "val_loss: 0.0016075725434347987, ATE: -0.023440733551979065, ATE_hat: 0.04404856264591217, loss: 0.10983574390411377, balance: 0.5263158082962036\n",
      "val_loss: 0.0022595322225242853, ATE: 0.0389520525932312, ATE_hat: 0.07671639323234558, loss: 0.10496068745851517, balance: 0.4736842215061188\n",
      "val_loss: 0.001619525020942092, ATE: 0.07791844010353088, ATE_hat: 0.11364533007144928, loss: 0.10758599638938904, balance: 0.48245614767074585\n",
      "val_loss: 0.0015441683353856206, ATE: 0.2110358476638794, ATE_hat: 0.13020887970924377, loss: 0.1629050076007843, balance: 0.5087719559669495\n",
      "val_loss: 0.001512173330411315, ATE: 0.10425388813018799, ATE_hat: 0.0537261962890625, loss: 0.19055166840553284, balance: 0.5263158082962036\n",
      "val_loss: 0.001823511440306902, ATE: 0.034657418727874756, ATE_hat: 0.1485954374074936, loss: 0.11480177938938141, balance: 0.44736841320991516\n",
      "val_loss: 0.0018062301678583026, ATE: 0.007231026887893677, ATE_hat: 0.10101145505905151, loss: 0.11240528523921967, balance: 0.5614035129547119\n",
      "val_loss: 0.0018082530004903674, ATE: 0.06895843148231506, ATE_hat: 0.07861447334289551, loss: 0.11723065376281738, balance: 0.6052631735801697\n",
      "val_loss: 0.0017039133235812187, ATE: 0.03972688317298889, ATE_hat: 0.05273938179016113, loss: 0.12714140117168427, balance: 0.5350877046585083\n",
      "val_loss: 0.001604930148459971, ATE: 0.11826883256435394, ATE_hat: 0.10482466220855713, loss: 0.18889202177524567, balance: 0.5\n",
      "val_loss: 0.001934607746079564, ATE: 0.19662870466709137, ATE_hat: 0.15944048762321472, loss: 0.13276855647563934, balance: 0.6228070259094238\n",
      "val_loss: 0.0020583404693752527, ATE: 0.17335808277130127, ATE_hat: 0.10766083002090454, loss: 0.14030958712100983, balance: 0.5175438523292542\n",
      "val_loss: 0.0019073581788688898, ATE: 0.1350427269935608, ATE_hat: 0.12592779099941254, loss: 0.157084122300148, balance: 0.4912280738353729\n",
      "val_loss: 0.0018375221407040954, ATE: 0.010145068168640137, ATE_hat: 0.05005180835723877, loss: 0.13629096746444702, balance: 0.5\n",
      "val_loss: 0.0017438980285078287, ATE: 0.2146589308977127, ATE_hat: 0.16336411237716675, loss: 0.14386090636253357, balance: 0.5175438523292542\n",
      "val_loss: 0.0017939932877197862, ATE: 0.1022036075592041, ATE_hat: 0.12864650785923004, loss: 0.12449699640274048, balance: 0.4736842215061188\n",
      "val_loss: 0.0014567265752702951, ATE: 0.014169037342071533, ATE_hat: 0.009166181087493896, loss: 0.14861071109771729, balance: 0.5\n",
      "val_loss: 0.0017784649971872568, ATE: 0.2240954488515854, ATE_hat: 0.1937641203403473, loss: 0.14480111002922058, balance: 0.5\n",
      "val_loss: 0.0017750991974025965, ATE: 0.05570483207702637, ATE_hat: 0.0571158230304718, loss: 0.1097797304391861, balance: 0.5263158082962036\n",
      "val_loss: 0.0017658283468335867, ATE: 0.03610748052597046, ATE_hat: 0.11523579061031342, loss: 0.12655757367610931, balance: 0.5175438523292542\n",
      "val_loss: 0.0017947994638234377, ATE: 0.08230288326740265, ATE_hat: 0.06670716404914856, loss: 0.08548504114151001, balance: 0.5438596606254578\n",
      "val_loss: 0.0017884598346427083, ATE: 0.07904857397079468, ATE_hat: 0.10118505358695984, loss: 0.12248474359512329, balance: 0.4912280738353729\n",
      "val_loss: 0.0017059784149751067, ATE: 0.13154250383377075, ATE_hat: 0.0730353593826294, loss: 0.1641794741153717, balance: 0.5614035129547119\n",
      "val_loss: 0.0016233028145506978, ATE: 0.25619232654571533, ATE_hat: 0.16787369549274445, loss: 0.17867016792297363, balance: 0.4912280738353729\n",
      "val_loss: 0.0018517988501116633, ATE: 0.13414542376995087, ATE_hat: 0.10069361329078674, loss: 0.12638327479362488, balance: 0.44736841320991516\n",
      "val_loss: 0.0019595474004745483, ATE: 0.04838249087333679, ATE_hat: 0.08154204487800598, loss: 0.1121424064040184, balance: 0.4649122953414917\n"
     ]
    }
   ],
   "source": [
    "cnn2.train(data_loader, epochs=200, validation_data = {\"X\":torch.tensor(X_val).float(), \n",
    "                                              \"tau\":torch.Tensor(tau_val).float()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cnn2.val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks without hidden layer, we can compare the coefficients to the known true coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09941928491355384"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09839207], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn2.net1.parameters())[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated coefficients usually capture the direction and even approximate size of the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01181628, -0.00056324, -0.00038409, -0.0026142 ,  0.00355688,\n",
       "       -0.01024895,  0.00623549,  0.00018308, -0.00534166, -0.01784753])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01961956,  0.00386629, -0.0082767 ,  0.00134611, -0.00027067,\n",
       "        -0.01149784,  0.00668612, -0.00531666, -0.00999668, -0.01973524]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnn2.net1.parameters())[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted treatment effects are usually not a accurate estimate of the true individual treatment effects, but correlation is often high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cnn2.net1(torch.tensor(X).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09954795307329109 0.02537136176155067\n",
      "0.09848804 0.033865847\n",
      "[[1.         0.90684453]\n",
      " [0.90684453 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tau.mean(), tau.std())\n",
    "print(pred.mean(), pred.std())\n",
    "print(np.corrcoef(tau,pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12902333, 0.11898324, 0.0572401 , 0.08173908, 0.14059615,\n",
       "       0.13592334, 0.11062738, 0.13640127, 0.09566238, 0.08779384],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12381572, 0.10763013, 0.04988213, 0.0977745 , 0.12228992,\n",
       "       0.13137319, 0.1038555 , 0.10988424, 0.09399612, 0.09210034])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
